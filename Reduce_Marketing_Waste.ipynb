{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv for train data\n",
    "data_train=pd.read_csv(\"../input/hackerearths-reduce-marketing-waste/train.csv\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking For Null Values if any Present\n",
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing extra special characters from columns for performing operations and coverting object to int or float according to convienence\n",
    "data_train['Deal_value'] = data_train['Deal_value'].str.replace('$', '')\n",
    "data_train['Weighted_amount'] = data_train['Weighted_amount'].str.replace('$', '')\n",
    "data_train['Deal_value']=data_train['Deal_value'].astype('float')\n",
    "data_train['Weighted_amount']=data_train['Weighted_amount'].astype('float')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling  Numerical null values by mean\n",
    "data_train['Deal_value'] = data_train['Deal_value'].fillna((data_train['Deal_value'].mean()))\n",
    "data_train['Weighted_amount'] = data_train['Weighted_amount'].fillna((data_train['Weighted_amount'].mean()))\n",
    "data_train['Internal_rating'] = data_train['Internal_rating'].fillna((data_train['Internal_rating'].mean()))\n",
    "data_train['Success_probability'] = data_train['Success_probability'].fillna((data_train['Success_probability'].mean()))\n",
    "data_train.head()\n",
    "#filling non numerical null values by most occuring one\n",
    "df_most_common_imputed = data_train.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "df_most_common_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding object data type to which it can be fitted to model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column_name in df_most_common_imputed.columns:\n",
    "    if df_most_common_imputed[column_name].dtype == object:\n",
    "        df_most_common_imputed[column_name] = le.fit_transform(df_most_common_imputed[column_name])\n",
    "    else:\n",
    "        pass\n",
    "#Selecting best featues according to importance\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "df_most_common_imputed[df_most_common_imputed < 0] = 0\n",
    "X = df_most_common_imputed.iloc[:,0:]  \n",
    "y = df_most_common_imputed.iloc[:,-1] \n",
    "\n",
    "y=y.astype('int')\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=23)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Features','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(23,'Score')) \n",
    "df_most_common_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop other columns other than top 5-6 selected\n",
    "df_most_common_imputed.drop([\"Deal_title\",\"Lead_name\",\"Date_of_creation\",\"Designation\",\"Pitch\",\"Industry\",\"Deal_value\",\"Contact_no\",\"Location\",\"POC_name\",\"Lead_POC_email\",\"Lead_revenue\",\"Resource\",\"Hiring_candidate_role\",\"Last_lead_update\",\"Internal_POC\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOXPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\n",
    "a=df_most_common_imputed['Weighted_amount']\n",
    "b=df_most_common_imputed['Fund_category']\n",
    "c=df_most_common_imputed['Geography']\n",
    "d=df_most_common_imputed['Lead_source']\n",
    "e=df_most_common_imputed['Level_of_meeting']\n",
    "f=df_most_common_imputed['Internal_rating']\n",
    "to_plot=[a,b,c,d,e,f]\n",
    "fig=plt.figure(1,figsize=(16,9))\n",
    "ax=fig.add_subplot(111)\n",
    "bp=ax.boxplot(to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #ScatterPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\n",
    "plt.scatter(b,c,e,f,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScatterPLOT FOR OUTLIAR DETECTION ON TRAINING DATA\n",
    "plt.scatter(a,d,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing Outliars from training data\n",
    "df_most_common_imputed\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "z_scores = stats.zscore(df_most_common_imputed)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "new_df = df_most_common_imputed[filtered_entries]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv for test data\n",
    "data_test=pd.read_csv(\"../input/hackerearths-reduce-marketing-waste/test.csv\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking For Null Values if any Present\n",
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing extra special characters from columns for performing operations and coverting object to int or float according to convienence\n",
    "data_test['Deal_value'] = data_test['Deal_value'].str.replace('$', '')\n",
    "data_test['Weighted_amount'] = data_test['Weighted_amount'].str.replace('$', '')\n",
    "data_test['Deal_value']=data_test['Deal_value'].astype('float')\n",
    "data_test['Weighted_amount']=data_test['Weighted_amount'].astype('float')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling  Numerical null values by mean\n",
    "data_test['Deal_value'] = data_test['Deal_value'].fillna((data_test['Deal_value'].mean()))\n",
    "data_test['Weighted_amount'] = data_test['Weighted_amount'].fillna((data_test['Weighted_amount'].mean()))\n",
    "data_test['Internal_rating'] = data_test['Internal_rating'].fillna((data_test['Internal_rating'].mean()))\n",
    "data_test.head()\n",
    "#filling non numerical null values by most occuring one\n",
    "dt_most_common_imputed = data_test.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "dt_most_common_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop other columns other than top 5-6 selected above\n",
    "dt_most_common_imputed.drop([\"Deal_title\",\"Lead_name\",\"Contact_no\",\"Deal_value\",\"Date_of_creation\",\"Designation\",\"Pitch\",\"Industry\",\"Location\",\"POC_name\",\"Lead_POC_email\",\"Lead_revenue\",\"Resource\",\"Hiring_candidate_role\",\"Last_lead_update\",\"Internal_POC\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding object data type to which it can be fitted to model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for column_name in dt_most_common_imputed.columns:\n",
    "    if dt_most_common_imputed[column_name].dtype == object:\n",
    "        dt_most_common_imputed[column_name] = le.fit_transform(dt_most_common_imputed[column_name])\n",
    "    else:\n",
    "        pass\n",
    "for column_name in df_most_common_imputed.columns:\n",
    "    if df_most_common_imputed[column_name].dtype == object:\n",
    "        df_most_common_imputed[column_name] = le.fit_transform(df_most_common_imputed[column_name])\n",
    "    else:\n",
    "        pass\n",
    "#splitting data for training and testing\n",
    "x_train = new_df.iloc[:,0:-1]  \n",
    "y_train = new_df.iloc[:,-1] \n",
    "x_test=   dt_most_common_imputed\n",
    "x_train\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOXPLOT FOR OUTLIAR DETECTION ON TESTING DATA\n",
    "a=dt_most_common_imputed['Weighted_amount']\n",
    "b=dt_most_common_imputed['Fund_category']\n",
    "c=dt_most_common_imputed['Geography']\n",
    "d=dt_most_common_imputed['Lead_source']\n",
    "e=dt_most_common_imputed['Level_of_meeting']\n",
    "f=dt_most_common_imputed['Internal_rating']\n",
    "to_plot=[a,b,c,d,e,f]\n",
    "fig=plt.figure(1,figsize=(16,9))\n",
    "ax=fig.add_subplot(111)\n",
    "bp=ax.boxplot(to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScatterPLOT FOR OUTLIAR DETECTION ON TESTING DATA\n",
    "plt.scatter(b,c,e,f,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ScatterPLOT FOR OUTLIAR DETECTION ON TESTING DATA\n",
    "plt.scatter(a,d,alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO OUTLIARS DETECTED NO NEED OF REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating different dataframe for output \n",
    "df=pd.DataFrame()\n",
    "df['Deal_title']=data_test['Deal_title']\n",
    "df['Success_probability']=df_most_common_imputed['Success_probability']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying xgboost as it is too efficient\n",
    "import xgboost as xgb\n",
    "xg_reg = xgb.XGBRegressor( learning_rate = 0.101,\n",
    "                max_depth =4,objective=\"reg:linear\",alpha =1,n_estimators=9)\n",
    "xg_reg.fit(x_train,y_train)\n",
    "xgbost= xg_reg.predict(x_test)\n",
    "df[\"Success_probability\"]=xgbost\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output CSV\n",
    "df.to_csv(\"xg_boost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
